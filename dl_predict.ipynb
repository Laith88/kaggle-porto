{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import BatchNormalization \n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(solution, submission):\n",
    "    df = zip(solution, submission, range(len(solution)))\n",
    "    df = sorted(df, key=lambda x: (x[1],-x[2]), reverse=True)\n",
    "    rand = [float(i+1)/float(len(df)) for i in range(len(df))]\n",
    "    totalPos = float(sum([x[0] for x in df]))\n",
    "    cumPosFound = [df[0][0]]\n",
    "    for i in range(1,len(df)):\n",
    "        cumPosFound.append(cumPosFound[len(cumPosFound)-1] + df[i][0])\n",
    "    Lorentz = [float(x)/totalPos for x in cumPosFound]\n",
    "    Gini = [Lorentz[i]-rand[i] for i in range(len(df))]\n",
    "    return sum(Gini)\n",
    "\n",
    "def normalized_gini(solution, submission):\n",
    "    normalized_gini = gini(solution, submission)/gini(solution, solution)\n",
    "    return normalized_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import keras.callbacks as kc\n",
    "\n",
    "class Metrics(kc.Callback):\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        predict = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        targ = self.validation_data[1]\n",
    "        self.ginis=(2*roc_auc_score(targ, predict))-1\n",
    "        return\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    non_imp = ['ps_ind_12_bin','ps_ind_13_bin','ps_ind_18_bin','ps_car_10_cat','ps_ind_11_bin','ps_ind_10_bin','ps_ind_14']\n",
    "    \n",
    "    train = (pd.read_csv('../data/train.csv', na_values=999)\n",
    "              .fillna(value=999))\n",
    "    unwanted = list((set(train.columns[train.columns.str.startswith('ps_calc_')])|set(non_imp)))\n",
    "    train.drop(unwanted, axis=1, inplace=True)\n",
    "            \n",
    "    test  = (pd.read_csv('../data/test.csv', na_values=999)\n",
    "              .fillna(value=999)\n",
    "              .drop(unwanted, axis=1)) \n",
    "            \n",
    "    X = train.drop(['id', 'target'], axis=1).values\n",
    "    y = train.target.values\n",
    "    test_id = test.id.values\n",
    "    test = test.drop('id', axis=1)\n",
    "\n",
    "    \n",
    "    f_dicts = make_dicts(train)\n",
    "\n",
    "    train_cat = np.array(train[f_dicts['type']['cat']])\n",
    "    train_cat[:,:] = np.add(train_cat[:,:],np.ones((train_cat.shape[0],train_cat.shape[1])))\n",
    "    test_cat  = np.array(test[f_dicts['type']['cat']])\n",
    "    test_cat[:,:] =  np.add(test_cat[:,:],np.ones((test_cat.shape[0],train_cat.shape[1])))\n",
    "\n",
    "    OH = OneHotEncoder()\n",
    "    OH.fit(np.array(list(train_cat) + list(test_cat)))\n",
    "    train_cat = OH.transform(train_cat).toarray()\n",
    "    test_cat = OH.transform(test_cat).toarray()\n",
    "\n",
    "    train_con = np.array(train[f_dicts['type']['con']])\n",
    "    test_con  = np.array(test[f_dicts['type']['con']])\n",
    "    RS = StandardScaler()\n",
    "    RS.fit(list(train_con) + list(test_con))\n",
    "    train_con = RS.transform(train_con)\n",
    "    train_con = RS.transform(train_con)\n",
    "\n",
    "    train_bin = np.array(train[f_dicts['type']['bin']])\n",
    "    test_bin = np.array(test[f_dicts['type']['bin']])\n",
    "\n",
    "\n",
    "    X = np.hstack((train_cat,train_con,train_bin))\n",
    "\n",
    "    X_test = np.hstack((test_cat,test_con,test_bin))\n",
    "    \n",
    "    smote = SMOTE(random_state=0)\n",
    "    X_resampled, y_resampled = smote.fit_sample(X, y)\n",
    "    \n",
    "    return X_resampled, y_resampled, X_test, test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dicts(df):\n",
    "    f_dicts ={}\n",
    "    \n",
    "    features_dict = {}\n",
    "    for x in ['ind', 'reg', 'car', 'calc']:\n",
    "        for y in ['cat', 'bin', 'con']:\n",
    "            features_dict[x+'_'+y] =[]\n",
    "            for i in df.columns.tolist()[2:]:\n",
    "                j = i.split('_')\n",
    "                if len(j) == 3:\n",
    "                    j.append('con')\n",
    "                if j[1]==x and j[3]==y:\n",
    "                    features_dict[x+'_'+y].append(i)\n",
    "    f_dicts['combo'] = features_dict\n",
    "    \n",
    "    features_dict_data_type = {}\n",
    "    for y in ['cat', 'bin', 'con']:\n",
    "        features_dict_data_type[y]=[]\n",
    "        for i in df.columns.tolist()[2:]:\n",
    "            j = i.split('_')\n",
    "            if len(j) == 3:\n",
    "                j.append('con')\n",
    "            if j[3]==y:\n",
    "                features_dict_data_type[y].append(i)\n",
    "    f_dicts['type'] = features_dict_data_type\n",
    "    \n",
    "    features_dict_data_label = {}\n",
    "    for x in ['ind', 'reg', 'car', 'calc']:\n",
    "        features_dict_data_label[x] =[]\n",
    "        for i in df.columns.tolist()[2:]:\n",
    "            j = i.split('_')\n",
    "            if j[1]==x:\n",
    "                features_dict_data_label[x].append(i)\n",
    "    f_dicts['label'] = features_dict_data_label\n",
    "    return f_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "    non_imp = ['ps_ind_12_bin','ps_ind_13_bin','ps_ind_18_bin','ps_car_10_cat','ps_ind_11_bin','ps_ind_10_bin','ps_ind_14']\n",
    "    \n",
    "    train = (pd.read_csv('../data/train.csv', na_values=999)\n",
    "              .fillna(value=999))\n",
    "    unwanted = list((set(train.columns[train.columns.str.startswith('ps_calc_')])|set(non_imp)))\n",
    "    train.drop(unwanted, axis=1, inplace=True)\n",
    "            \n",
    "    test  = (pd.read_csv('../data/test.csv', na_values=999)\n",
    "              .fillna(value=999)\n",
    "              .drop(unwanted, axis=1)) \n",
    "    \n",
    "    y = train.target.values        \n",
    "    train = train.drop(['id', 'target'], axis=1)\n",
    "    \n",
    "    test_id = test.id.values\n",
    "    test = test.drop('id', axis=1)\n",
    "        \n",
    "    return train.values, y, test.values, test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=198, kernel_initializer='glorot_uniform',bias_initializer='zeros', activation='sigmoid'))\n",
    "    model.add(Dense(256, kernel_initializer='glorot_uniform', activation='sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, kernel_initializer='glorot_uniform', activation='sigmoid'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.02),metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define base model\n",
    "def optimized_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=198, kernel_initializer='glorot_uniform',bias_initializer='zeros', activation='sigmoid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(256, kernel_initializer='glorot_uniform', activation='tanh'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.01))\n",
    "    model.add(Dense(128, kernel_initializer='glorot_uniform', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(256, kernel_initializer='glorot_uniform', activation='tanh'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, kernel_initializer='glorot_uniform', activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.02),metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = optimized_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, X_test, test_id = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/6]\n",
      "Train on 955862 samples, validate on 191174 samples\n",
      "Epoch 1/20\n",
      "955862/955862 [==============================] - 89s - loss: 0.6945 - acc: 0.5038 - val_loss: 0.6976 - val_acc: 0.5007\n",
      "Epoch 2/20\n",
      "955862/955862 [==============================] - 91s - loss: 0.6936 - acc: 0.5002 - val_loss: 0.6936 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "955862/955862 [==============================] - 88s - loss: 0.6935 - acc: 0.4995 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "955862/955862 [==============================] - 89s - loss: 0.6935 - acc: 0.5006 - val_loss: 0.7006 - val_acc: 0.5001\n",
      "Epoch 00003: early stopping\n",
      "-0.0004184424303\n",
      "[Fold 1/6 Prediciton:]\n",
      "269696/892816 [========>.....................] - ETA: 16s"
     ]
    }
   ],
   "source": [
    "kfold = 6\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=42)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_id\n",
    "sub['target'] = np.zeros_like(test_id)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print('[Fold %d/%d]' % (i + 1, kfold))\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    #class_weight = {1 : (len(y_train) - sum(y_train))/y_train.shape[0], 0: sum(y_train)/y_train.shape[0]}\n",
    "    ES = EarlyStopping(monitor='val_loss', min_delta=0, patience=1, verbose=1, mode='auto')\n",
    "    history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,validation_data=(X_valid,y_valid), callbacks=[ES,metrics])#, class_weight = class_weight\n",
    "    print(metrics.ginis)\n",
    "    print('[Fold %d/%d Prediciton:]' % (i + 1, kfold))\n",
    "    # Predict on our test data\n",
    "    p_test =model.predict_proba(X_test)[:,0]\n",
    "    sub['target'] += p_test/kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'DL_model_'+str(dt.datetime.now()).replace(' ','_').replace(':','').replace('.','')\n",
    "sub.id = sub.id.astype('Int32')\n",
    "sub.to_csv('../output/'+filename+'.csv', index=False)\n",
    "#test = np.load('../output/params.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
