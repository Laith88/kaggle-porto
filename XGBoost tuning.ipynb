{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pylab as plt\n",
    "from itertools import combinations\n",
    "from scipy.stats import uniform\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 24, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(solution, submission):\n",
    "    df = zip(solution, submission, range(len(solution)))\n",
    "    df = sorted(df, key=lambda x: (x[1],-x[2]), reverse=True)\n",
    "    rand = [float(i+1)/float(len(df)) for i in range(len(df))]\n",
    "    totalPos = float(sum([x[0] for x in df]))\n",
    "    cumPosFound = [df[0][0]]\n",
    "    for i in range(1,len(df)):\n",
    "        cumPosFound.append(cumPosFound[len(cumPosFound)-1] + df[i][0])\n",
    "    Lorentz = [float(x)/totalPos for x in cumPosFound]\n",
    "    Gini = [Lorentz[i]-rand[i] for i in range(len(df))]\n",
    "    return sum(Gini)\n",
    "\n",
    "def normalized_gini(solution, submission):\n",
    "    normalized_gini = gini(solution, submission)/gini(solution, solution)\n",
    "    return normalized_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dicts(df):\n",
    "    f_dicts ={}\n",
    "    \n",
    "    features_dict = {}\n",
    "    for x in ['ind', 'reg', 'car', 'calc']:\n",
    "        for y in ['cat', 'bin', 'con']:\n",
    "            features_dict[x+'_'+y] =[]\n",
    "            for i in df.columns.tolist()[2:]:\n",
    "                j = i.split('_')\n",
    "                if len(j) == 3:\n",
    "                    j.append('con')\n",
    "                if j[1]==x and j[3]==y:\n",
    "                    features_dict[x+'_'+y].append(i)\n",
    "    f_dicts['combo'] = features_dict\n",
    "    \n",
    "    features_dict_data_type = {}\n",
    "    for y in ['cat', 'bin', 'con']:\n",
    "        features_dict_data_type[y]=[]\n",
    "        for i in df.columns.tolist()[2:]:\n",
    "            j = i.split('_')\n",
    "            if len(j) == 3:\n",
    "                j.append('con')\n",
    "            if j[3]==y:\n",
    "                features_dict_data_type[y].append(i)\n",
    "    f_dicts['type'] = features_dict_data_type\n",
    "    \n",
    "    features_dict_data_label = {}\n",
    "    for x in ['ind', 'reg', 'car', 'calc']:\n",
    "        features_dict_data_label[x] =[]\n",
    "        for i in df.columns.tolist()[2:]:\n",
    "            j = i.split('_')\n",
    "            if j[1]==x:\n",
    "                features_dict_data_label[x].append(i)\n",
    "    f_dicts['label'] = features_dict_data_label\n",
    "    return f_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    #non_imp = ['ps_ind_16_bin','ps_car_08_cat','ps_car_11_cat','ps_ind_06_bin','ps_ind_12_bin','ps_ind_13_bin','ps_car_02_cat','ps_ind_07_bin','ps_ind_09_bin','ps_ind_08_bin','ps_ind_14','ps_ind_18_bin','ps_car_10_cat','ps_ind_11_bin','ps_ind_10_bin']\n",
    "    \n",
    "    train = (pd.read_csv('../data/train.csv', na_values=999)\n",
    "              .fillna(value=999))\n",
    "    #unwanted = list((set(train.columns[train.columns.str.startswith('ps_calc_')])|set(non_imp)))\n",
    "    #unwanted = ['ps_ind_10_bin','ps_ind_11_bin','ps_ind_12_bin','ps_ind_13_bin','ps_car_10_cat','ps_ind_14','ps_car_11_cat',\n",
    "    #           'ps_car_02_cat','ps_car_08_cat','ps_ind_18_bin','ps_ind_08_bin','ps_calc_15_bin','ps_calc_20_bin']\n",
    "    unwanted =  ['ps_calc_19_bin',\n",
    "                 'ps_car_10_cat',\n",
    "                 'ps_ind_11_bin',\n",
    "                 'ps_calc_06',\n",
    "                 'ps_calc_08',\n",
    "                 'ps_calc_13',\n",
    "                 'ps_calc_11',\n",
    "                 'ps_calc_07',\n",
    "                 'ps_calc_18_bin',\n",
    "                 'ps_calc_04',\n",
    "                 'ps_calc_14',\n",
    "                 'ps_calc_01',\n",
    "                 'ps_calc_17_bin',\n",
    "                 'ps_calc_09',\n",
    "                 'ps_calc_02',\n",
    "                 'ps_ind_13_bin',\n",
    "                 'ps_ind_18_bin',\n",
    "                 'ps_calc_16_bin',\n",
    "                 'ps_ind_09_bin',\n",
    "                 'ps_calc_03',\n",
    "                 'ps_ind_10_bin',\n",
    "                 'ps_calc_05',\n",
    "                 'ps_ind_14',\n",
    "                 'ps_calc_15_bin',\n",
    "                 'ps_ind_12_bin',\n",
    "                 'ps_calc_12',\n",
    "                 'ps_calc_10',\n",
    "                 'ps_calc_20_bin']\n",
    "    train.drop(unwanted, axis=1, inplace=True)\n",
    "            \n",
    "    test  = (pd.read_csv('../data/test.csv', na_values=999)\n",
    "              .fillna(value=999)\n",
    "              .drop(unwanted, axis=1)) \n",
    "    \n",
    "    y = train.target.values        \n",
    "    train = train.drop(['id', 'target'], axis=1)\n",
    "    \n",
    "    test_id = test.id.values\n",
    "    test = test.drop('id', axis=1)\n",
    "    \n",
    "    ## interactions:\n",
    "    interaction_features = ['ps_car_13', 'ps_reg_03', 'ps_car_14', 'ps_reg_02', 'ps_ind_15', 'ps_ind_03']\n",
    "    feature_combintation  = list(combinations(interaction_features, 2))\n",
    "    for first_feature, second_feature in feature_combintation:\n",
    "        new_name  = first_feature+'_'+second_feature+'comb'\n",
    "        train[new_name] = train[first_feature].values*train[second_feature].values\n",
    "        test[new_name] = test[first_feature].values*test[second_feature].values\n",
    "\n",
    "    fl = train.columns.tolist()\n",
    "    \n",
    "    return train.values, y, test.values, test_id, fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    train, labels, _, _ = prepare_data()\n",
    "    # Create the pipeline\n",
    "\n",
    "    # Create a parameter grid to search for best parameters for everything in the pipeline\n",
    "    param_grid = {\n",
    "                    'min_child_weight': [11], # due to high class imbalance\n",
    "                    'objective': ['binary:logistic'],\n",
    "                    'max_depth': [8],\n",
    "                    'nthread': [12],\n",
    "                    'max_delta_step': [1.8],\n",
    "                    'colsample_bytree': [0.4],\n",
    "                    'subsample': [0.88],\n",
    "                    'learning_rate': [0.025], #0.025\n",
    "                    'gamma': [i/10.0 for i in range(0,10)],\n",
    "                    'n_estimators' : [260],\n",
    "                    #'tree_method': ['gpu_hist'],\n",
    "                    'silent': [1],\n",
    "                    'missing':[-999],\n",
    "                    'n_jobs': [12],\n",
    "                    #'updater': 'grow_gpu',\n",
    "                    'scale_pos_weight' : [1] # due to high class imbalance\n",
    "                  }\n",
    "\n",
    "    # Normalized Gini Scorer\n",
    "    gini_scorer = metrics.make_scorer(normalized_gini, greater_is_better = True)\n",
    "\n",
    "    # Initialize Grid Search Model\n",
    "    model = GridSearchCV(estimator  = xgb.XGBClassifier(),\n",
    "                         param_grid = param_grid,\n",
    "                         scoring    = 'roc_auc',\n",
    "                         verbose    = 2,\n",
    "                         n_jobs     = 12,\n",
    "                         iid        = True,\n",
    "                         refit      = True,\n",
    "                         cv         = StratifiedKFold(labels, n_folds=5, shuffle=True))\n",
    "    # Fit Grid Search Model\n",
    "    model.fit(train, labels)\n",
    "    print(\"Best score: %0.3f\" % model.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = model.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "    # Get best model\n",
    "    best_model = model.best_estimator_\n",
    "\n",
    "    # Fit model with best parameters optimized for normalized_gini\n",
    "    #best_model.fit(train,labels)\n",
    "\n",
    "    return best_model, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb.XGBClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_rand():\n",
    "    train, labels, _, _, _ = prepare_data()\n",
    "    # Create the pipeline\n",
    "    # Create a parameter grid to search for best parameters for everything in the pipeline\n",
    "    param_grid = {\n",
    "                    'min_child_weight': [10,11,12], # due to high class imbalance\n",
    "                    'objective': ['binary:logistic'],\n",
    "                    'max_depth': [7,8,9],\n",
    "                    'nthread': [12],\n",
    "                    'max_delta_step': [1.8],\n",
    "                    'colsample_bytree': [0.51880508157068184],\n",
    "                    'subsample': [0.69706874031839083],\n",
    "                    'learning_rate': [0.024],#[0.028084362074445819], #0.025\n",
    "                    'gamma': [0.068326902334014492], #np.random.uniform(0.066,.07,1000),\n",
    "                    'n_estimators' : [350],\n",
    "                    'reg_alpha':[0.00975],#[0.01],\n",
    "                    #'tree_method': ['gpu_hist'],\n",
    "                    'silent': [1],\n",
    "                    'missing':[999],\n",
    "                    'n_jobs': [12],\n",
    "                    #'updater': 'grow_gpu',\n",
    "                    'scale_pos_weight' : [1,2,3] # due to high class imbalance\n",
    "                  }\n",
    "\n",
    "    # Normalized Gini Scorer\n",
    "    #gini_scorer = metrics.make_scorer(normalized_gini, greater_is_better = True)\n",
    "\n",
    "    # Initialize Grid Search Model\n",
    "    model = RandomizedSearchCV(estimator  = xgb.XGBClassifier(),\n",
    "                         param_distributions = param_grid,\n",
    "                         #param_grid = param_grid,\n",
    "                         scoring    = 'roc_auc',\n",
    "                         n_iter     = 10,     \n",
    "                         verbose    = 2,\n",
    "                         n_jobs     = 12,\n",
    "                         iid        = True,\n",
    "                         refit      = True,\n",
    "                         cv         = StratifiedKFold( n_splits=2, shuffle=True))\n",
    "    # Fit Grid Search Model\n",
    "    model.fit(train, labels)\n",
    "    print(\"Best score: %0.3f\" % model.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = model.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "    # Get best model\n",
    "    best_model = model.best_estimator_\n",
    "\n",
    "    # Fit model with best parameters optimized for normalized_gini\n",
    "    #best_model.fit(train,labels)\n",
    "\n",
    "    return best_model, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "bstmdl, grd = test_rand()\n",
    "print()\n",
    "print('this took {}-seconds'.format(time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
